{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\nfrom tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\nfrom tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textwrap import wrap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:07:34.170807Z","iopub.execute_input":"2025-12-17T19:07:34.171566Z","iopub.status.idle":"2025-12-17T19:07:34.177708Z","shell.execute_reply.started":"2025-12-17T19:07:34.171533Z","shell.execute_reply":"2025-12-17T19:07:34.177066Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"image_path = '/kaggle/input/flickr8k/Images'\ndata = pd.read_csv(\"/kaggle/input/flickr8k/captions.txt\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:07:38.255661Z","iopub.execute_input":"2025-12-17T19:07:38.255927Z","iopub.status.idle":"2025-12-17T19:07:38.313854Z","shell.execute_reply.started":"2025-12-17T19:07:38.255905Z","shell.execute_reply":"2025-12-17T19:07:38.313205Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import re\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].str.lower()\n    data['caption'] = data['caption'].str.replace(r'[^a-z]', ' ', regex=True)\n    data['caption'] = data['caption'].str.replace(r'\\s+', ' ', regex=True)\n    data['caption'] = data['caption'].apply(\n        lambda x: \" \".join([word for word in x.split() if len(word) > 1])\n    )\n    data['caption'] = 'startseq ' + data['caption'] + ' endseq'\n    return data\n\ndata = text_preprocessing(data)\n\ncaptions = data['caption'].tolist()\n\ncaptions[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:11:27.399365Z","iopub.execute_input":"2025-12-17T19:11:27.399971Z","iopub.status.idle":"2025-12-17T19:11:27.873299Z","shell.execute_reply.started":"2025-12-17T19:11:27.399942Z","shell.execute_reply":"2025-12-17T19:11:27.872598Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['startseq startseq startseq child in pink dress is climbing up set of stairs in an entry way endseq endseq endseq',\n 'startseq startseq startseq girl going into wooden building endseq endseq endseq',\n 'startseq startseq startseq little girl climbing into wooden playhouse endseq endseq endseq',\n 'startseq startseq startseq little girl climbing the stairs to her playhouse endseq endseq endseq',\n 'startseq startseq startseq little girl in pink dress going into wooden cabin endseq endseq endseq',\n 'startseq startseq startseq black dog and spotted dog are fighting endseq endseq endseq',\n 'startseq startseq startseq black dog and tri colored dog playing with each other on the road endseq endseq endseq',\n 'startseq startseq startseq black dog and white dog with brown spots are staring at each other in the street endseq endseq endseq',\n 'startseq startseq startseq two dogs of different breeds looking at each other on the road endseq endseq endseq',\n 'startseq startseq startseq two dogs on pavement moving toward each other endseq endseq endseq']"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index)+1;\n\nimport pickle\n\n# Save the tokenizer\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n\n# Save the feature extractor model\nfe.save(\"feature_extractor.keras\")\n\n\nmax_length = max(len(caption.split()) for caption in captions)\n\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85*nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:57:39.538416Z","iopub.execute_input":"2025-12-17T19:57:39.539033Z","iopub.status.idle":"2025-12-17T19:57:40.121752Z","shell.execute_reply.started":"2025-12-17T19:57:39.539001Z","shell.execute_reply":"2025-12-17T19:57:40.120827Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"model = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    img = np.expand_dims(img,axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:30:40.029364Z","iopub.execute_input":"2025-12-17T19:30:40.030009Z","iopub.status.idle":"2025-12-17T19:44:20.659865Z","shell.execute_reply.started":"2025-12-17T19:30:40.029969Z","shell.execute_reply":"2025-12-17T19:44:20.659044Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1765999840.759203     193 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels.h5\n\u001b[1m82524592/82524592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/8091 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1765999852.840233     484 service.cc:152] XLA service 0x7d9ae8004750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1765999852.840271     484 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1765999855.673634     484 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1765999871.406733     484 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100%|██████████| 8091/8091 [13:33<00:00,  9.94it/s]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"\n\nclass CustomDataGenerator(Sequence):\n    \n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n                 vocab_size, max_length, features,shuffle=True):\n    \n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __len__(self):\n        return self.n // self.batch_size\n    \n    def __getitem__(self,index):\n    \n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n        X1, X2, y = self.__get_data(batch)        \n        return (X1, X2), y\n    \n    def __get_data(self,batch):\n        \n        X1, X2, y = list(), list(), list()\n        \n        images = batch[self.X_col].tolist()\n           \n        for image in images:\n            feature = self.features[image][0]\n            \n            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1,len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            \n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                \n        return X1, X2, y\n\n\ntrain_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n\nvalidation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:57:47.582000Z","iopub.execute_input":"2025-12-17T19:57:47.582575Z","iopub.status.idle":"2025-12-17T19:57:47.596133Z","shell.execute_reply.started":"2025-12-17T19:57:47.582545Z","shell.execute_reply":"2025-12-17T19:57:47.595197Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\ninput1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\nmerged = concatenate([img_features_reshaped,sentence_features],axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1,input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy',optimizer='adam')\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Define the model checkpoint\nmodel_name = \"model.keras\"  # Update the extension to .keras\ncheckpoint = ModelCheckpoint(\n    model_name,\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_best_only=True,\n    save_weights_only=False,\n    verbose=1\n)\n\nearlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.00000001)\n\n\n\n\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint,earlystopping,learning_rate_reduction])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:57:51.110666Z","iopub.execute_input":"2025-12-17T19:57:51.110966Z","iopub.status.idle":"2025-12-17T20:17:51.219911Z","shell.execute_reply.started":"2025-12-17T19:57:51.110940Z","shell.execute_reply":"2025-12-17T20:17:51.219240Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 4.4103\nEpoch 1: val_loss improved from inf to 3.13471, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 176ms/step - loss: 4.4090 - val_loss: 3.1347 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 3.1584\nEpoch 2: val_loss improved from 3.13471 to 2.89279, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 174ms/step - loss: 3.1583 - val_loss: 2.8928 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 2.9271\nEpoch 3: val_loss improved from 2.89279 to 2.77988, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 171ms/step - loss: 2.9271 - val_loss: 2.7799 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 2.7759\nEpoch 4: val_loss improved from 2.77988 to 2.70877, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 173ms/step - loss: 2.7759 - val_loss: 2.7088 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 2.6755\nEpoch 5: val_loss improved from 2.70877 to 2.66803, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 170ms/step - loss: 2.6755 - val_loss: 2.6680 - learning_rate: 0.0010\nEpoch 6/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 2.5989\nEpoch 6: val_loss improved from 2.66803 to 2.64096, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 171ms/step - loss: 2.5989 - val_loss: 2.6410 - learning_rate: 0.0010\nEpoch 7/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 2.5452\nEpoch 7: val_loss improved from 2.64096 to 2.63061, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 172ms/step - loss: 2.5452 - val_loss: 2.6306 - learning_rate: 0.0010\nEpoch 8/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 2.4870\nEpoch 8: val_loss improved from 2.63061 to 2.61346, saving model to model.keras\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 171ms/step - loss: 2.4870 - val_loss: 2.6135 - learning_rate: 0.0010\nEpoch 9/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 2.4426\nEpoch 9: val_loss did not improve from 2.61346\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 169ms/step - loss: 2.4426 - val_loss: 2.6191 - learning_rate: 0.0010\nEpoch 10/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 2.3993\nEpoch 10: val_loss did not improve from 2.61346\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 169ms/step - loss: 2.3993 - val_loss: 2.6244 - learning_rate: 0.0010\nEpoch 11/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 2.3703\nEpoch 11: val_loss did not improve from 2.61346\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 167ms/step - loss: 2.3703 - val_loss: 2.6344 - learning_rate: 0.0010\nEpoch 12/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 2.2982\nEpoch 12: val_loss did not improve from 2.61346\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 173ms/step - loss: 2.2982 - val_loss: 2.6298 - learning_rate: 2.0000e-04\nEpoch 13/50\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 2.2752\nEpoch 13: val_loss did not improve from 2.61346\n\u001b[1m537/537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 173ms/step - loss: 2.2752 - val_loss: 2.6356 - learning_rate: 2.0000e-04\nEpoch 13: early stopping\nRestoring model weights from the end of the best epoch: 8.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import pickle\n\n# Save the tokenizer\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n\n# Save the feature extractor model\nfe.save(\"feature_extractor.keras\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T20:20:39.989379Z","iopub.execute_input":"2025-12-17T20:20:39.990219Z","iopub.status.idle":"2025-12-17T20:20:42.159653Z","shell.execute_reply.started":"2025-12-17T20:20:39.990184Z","shell.execute_reply":"2025-12-17T20:20:42.158810Z"}},"outputs":[],"execution_count":30}]}